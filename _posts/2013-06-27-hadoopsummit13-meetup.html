---
layout: post
title: hadoopsummit13 meetup
date: 2013-06-27 21:17:49.000000000 -07:00
categories: []
tags: []
status: publish
type: post
published: true
meta:
  _publicize_pending: '1'
author: 
---
<div style="text-align:justify;">Yesterday, I attended a meet-up session on big data and machine learning. Hadoop summit 2013 is kicking off in San Jose and event organizers were able use it as an excuse catch hold of some big names/vendors in this field.</p>
<p>[caption id="attachment_165" align="aligncenter" width="526"]<img class=" wp-image-165 " alt="ted dunning" src="/assets/photo-4.jpg" width="526" height="374" /> Ted Dunning on Apache Mahout[/caption]</p>
</div>
<div style="text-align:justify;"><span style="font-style:inherit;line-height:1.625;">The first speaker for the night was Ted Dunning, who as everyone knows is guru in this field. He started off with an introduction on <a href="http://mahout.apache.org/" target="_blank">Apache Mahout</a>, pointing out areas where Mahout is good and comparable to best performing implementations in other platforms. He spoke about different packages Mahout provides and how to utilise them best. For example Recommendation package has plethora of good online algorithms, but it performs poorly in classification tasks. He also spoke about math library in java, which can be used to do all vector/matrix manipulations like Python or Matlab. He also mentioned that these algorithms have both in memory and distributed implementation, so that will be something cool to checkout. </span><a style="font-style:inherit;line-height:1.625;" title="tdunning slideshare" href="http://www.slideshare.net/tdunning/whats-right-and-wrong-with-apache-mahout." target="_blank">Link to his slides</a><span style="font-style:inherit;line-height:1.625;">.</span></div>
<div style="text-align:justify;">Second talk was from <a title="alpine data labs" href="http://www.alpinedatalabs.com/" target="_blank">Alpine data labs</a> which sounded almost like a sales pitch to me. They showed their parallel implementation of SVM where the key was to apply an approximation technique to one of the computation of Lagrange multiplier coefficients. It was a good descriptive talk and got many people thinking about the inherent details of the algorithm.</div>
<div style="text-align:justify;"><a title="0xdata" href="http://0xdata.com/" target="_blank">0xdata</a> started off with the theme of how they want to bring data science to masses and  help them get away from the direct confrontation with mathematics. Their product can interface with disparate sources like excel, R, SAS and extend the in memory implementations on to the distributed platform. They worked through an interesting proof of concept using a on-time-airline dataset <a href="http://stat-computing.org/dataexpo/2009/" target="_blank">http://stat-computing.org/dataexpo/2009/</a>.</div>
